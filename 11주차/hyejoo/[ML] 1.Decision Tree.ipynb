{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decision Tree (의사결정나무)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Decision Tree의 장단점\n",
    "\n",
    "1) 장점\n",
    " - 결과 해석의 용이성\n",
    " - 비모수적 모델 : 통계적 가정으로부터 자유로움\n",
    " - Scaling이 필요하지 않음 (서수 기반)\n",
    " - 이상치에 민감하지 않음\n",
    " - 변수 간 상호작용 고려 가능\n",
    " - 분류, 회귀 모두 가능\n",
    " \n",
    "\n",
    "2) 단점\n",
    " - 불안정성 : 데이터 크기가 작을 경우 불안정성이 높아짐\n",
    " - Overfitting\n",
    " - 선형관계 파악 미흡\n",
    " - 비연속성 : 연속형 변수 구간화하여 처리. 분리 경계점 근처에서 오류 발생 가능성 있음.\n",
    " - 높은 Computational Cost : 각 조합의 결과를 모두 고려할 경우\n",
    " \n",
    "### 2. Decision Tree 분할 기준 (Impurity, Feature Importance)\n",
    "\n",
    "1) Classification\n",
    " - Gini Index\n",
    " - Entropy (Shannon's Entropy 등)\n",
    " - Information Gain Ratio\n",
    " \n",
    "2) Regression\n",
    " - MSE (Mean Square Error)\n",
    " - MAE (Mean Absolute Error)\n",
    " - Reduction in Possion Deviance\n",
    " - Friedman's MSE\n",
    "\n",
    "### 3. Decision Tree의 모델 구분 기준\n",
    "\n",
    "1) 분할(평가지표) 기준 (분산이 어떻게 계산되는가?)  \n",
    "2) 종속변수 기준 (Classification / Regression)\n",
    "3) 과적합 방지 기준\n",
    "4) 불완전 데이터 적용 가능 여부\n",
    "\n",
    "### 4. Decision Tree Algorithm 종류\n",
    "\n",
    "#### Gini Index 기반\n",
    "1) CART (Classification And Regression Tree) : 분류/회귀 모두 가능. 이진 분할\n",
    "\n",
    "#### Entropy 기반\n",
    "2) ID3 (Iterative Dichotomizer) : Shannon Entropy 사용 (즉, Information Gain 사용). 분류모델만 가능.\n",
    "3) C4.5 : Information Gain Ratio 사용. 불완전 데이터를 처리할 수 있으며 (결측치 포함 데이터 처리 가능), Pruning이 가능하여 Overfitting 방지 가능.\n",
    "4) C5.0 : C4.5의 발전된 형태. 메모리 효율성 등 성능이 향상됨.\n",
    "\n",
    "#### 기타\n",
    "5) CHAID (Chi-Square Automatic Interaction Detection) : Chai-Square Statistics (범주형), F-statistics (연속형) 사용. 다지분리 (Multiway Split) 수행.\n",
    "\n",
    "### 4. 참고 자료\n",
    "https://heytech.tistory.com/145\n",
    "https://yschoi.pusan.ac.kr/sites/yschoi/download/DataMining/2-5.htm\n",
    "http://ducj3.iptime.org/decisiontree/\n",
    "https://tyami.github.io/machine%20learning/decision-tree-3-c4_5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 설정\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sklearn as sk\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib 사용 시 한글 깨짐 문제 해결\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "font_path = \"C:/Windows/Fonts/NGULIM.ttf\"\n",
    "font = font_manager.FontProperties(fname = font_path).get_name()\n",
    "rc('font', family = font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경고 메시지 비활성화\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\python.exe\n",
      "['C:\\\\Users\\\\lsc\\\\Desktop\\\\ADP', 'D:\\\\Anaconda3\\\\python310.zip', 'D:\\\\Anaconda3\\\\DLLs', 'D:\\\\Anaconda3\\\\lib', 'D:\\\\Anaconda3', '', 'D:\\\\Anaconda3\\\\lib\\\\site-packages', 'D:\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32', 'D:\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'D:\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('D:/Anaconda3/Lib/site-packages/Graphviz/bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path[-1] = sys.path[-1].replace(\"/\", \"\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\lsc\\\\Desktop\\\\ADP', 'D:\\\\Anaconda3\\\\python310.zip', 'D:\\\\Anaconda3\\\\DLLs', 'D:\\\\Anaconda3\\\\lib', 'D:\\\\Anaconda3', '', 'D:\\\\Anaconda3\\\\lib\\\\site-packages', 'D:\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32', 'D:\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'D:\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'D:\\\\Anaconda3\\\\Lib\\\\site-packages\\\\Graphviz\\\\bin']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_iris\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m system\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# system(\"pip install graphviz\")\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# system(\"pip show graphviz\")\u001b[39;00m\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;66;03m# Colab에는 이미 설치되어 있음.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m iris \u001b[38;5;241m=\u001b[39m load_iris()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.datasets import load_iris\n",
    "from os import system\n",
    "import graphviz\n",
    "# system(\"pip install graphviz\")\n",
    "# system(\"pip show graphviz\")\n",
    "  # Colab에는 이미 설치되어 있음.\n",
    "\n",
    "iris = load_iris()\n",
    "print(\"iris dataset attributes : \", dir(iris))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_x = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "iris_x.columns = [colnames.replace(\" (cm)\", \"\") for colnames in iris_x.columns]\n",
    "iris_x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_y = pd.DataFrame(iris.target, columns = [\"species\"])\n",
    "iris_y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names\n",
    "  # Species. 분석 종료 후 범주형으로 변경해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X 결측치 : \", iris_x.isnull().sum().sum(), \"개\")\n",
    "print(\"y 결측치 : \", iris_y.isnull().sum().sum(), \"개\")\n",
    "  # 결측치가 없는 예쁜 데이터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_y.value_counts()\n",
    "  # Balanced Data이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Decision Tree Classifier : 의사결정나무 분류기 (CART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf2 = DecisionTreeClassifier(criterion = \"entropy\")\n",
    "clf2.fit(iris_x, iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = DecisionTreeClassifier()\n",
    "    # Criterion (Information Gain 방식) 기본값 : Gini Index\n",
    "clf3.fit(iris_x, iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 의사결정나무 분류기 시각화 1 (Entropy)\n",
    "import graphviz\n",
    "    # Graphviz : Tree를 도식화하는 라이브러리.\n",
    "\n",
    "dot_data = tree.export_graphviz(clf2, out_file = None\n",
    "                                     , feature_names = iris.feature_names\n",
    "                                     , class_names = iris.target_names\n",
    "                                     , filled = True  # 색칠 여부\n",
    "                                     , rounded = True  # 반올림 여부\n",
    "                                     , special_characters = True  # 특수문자 사용 여부\n",
    ")\n",
    "\n",
    "graph = graphviz.Source(dot_data)              \n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 의사결정나무 분류기 시각화 2 (Gini Index)\n",
    "\n",
    "dot_data_2 = tree.export_graphviz(clf3, out_file = None\n",
    "                                     , feature_names = iris.feature_names\n",
    "                                     , class_names = iris.target_names\n",
    "                                     , filled = True  # 색칠 여부\n",
    "                                     , rounded = True  # 반올림 여부\n",
    "                                     , special_characters = True  # 특수문자 사용 여부\n",
    ")\n",
    "\n",
    "graph_2 = graphviz.Source(dot_data_2)              \n",
    "graph_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1-1. Impurity (불순도) 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree의 분지(Split)를 결정하는 수단\n",
    "\n",
    "Class가 잘 분류되어 있는 경우 불순도가 낮음.  \n",
    "즉, 불순도 감소량이 최대가 되는 방향으로 분지하게 된다.\n",
    "\n",
    "$$ I(T') = Weighted \\; Impurity \\; after \\; split $$\n",
    "\n",
    "$$ \\Delta I(s, t) = I(T) - I(T') $$\n",
    "\n",
    "$$ argmax \\Delta I(s, t) $$\n",
    "\n",
    "\n",
    "#### 1) Gini Index\n",
    "\n",
    " - 노드 t의 지니 지수 : $$ Gini(t) = 1 - \\Sigma_{j}^{J}[p(j | t)]^2$$\n",
    "\n",
    " - Split (파티션)의 지니 지수 : $$ Gini_{split} = \\Sigma_{i}^k {n_i \\over n} Gini(i)$$\n",
    "\n",
    "$$ n_i : 자식 \\; 노드의 \\; 크기 $$\n",
    "$$ n : 부모 \\; 노드의 \\; 크기 $$\n",
    "\n",
    "Tree의 Gini Index는 Terminal Node들의 Gini Index 값의 Weighted Sum이다.\n",
    "\n",
    "\n",
    "#### 2) Entropy\n",
    "\n",
    "$$ Entropy(t) = -\\Sigma_{j}^{J} p(j | t) log_2 p(j|t)$$\n",
    "\n",
    "\n",
    "#### 3) Information Gain\n",
    "\n",
    "$$ IG = Base \\; Entropy - New \\; Entropy $$\n",
    "$$ = \\Delta I(s, t) = I(T) - I(T')$$\n",
    "\n",
    "불순도 감소량과 같다. 즉, 불순도 감소량을 최대화하는 것은 정보 이득을 최대화하는 것과 일맥상통한다.\n",
    "\n",
    " \n",
    "#### 4) IG Ratio (Information Gain Ratio)\n",
    "\n",
    "그럼에도 불구하고, Information Gain 극대화에만 초점을 두면 안된다.\n",
    "\n",
    "Information Gain이 극도로 커진다는 것은 Tree가 끝없이 커져 전체 Entropy가 0이 된다는 것을 의미하며, Train Data에 대한 Overfitting 문제가 생기게 되기 때문이다.\n",
    "\n",
    "이를 보완하는 개념 중 하나가 IG Ratio이다.\n",
    "\n",
    "\n",
    "$$ IGR = {IG \\over {New \\; Entropy}} = {(Base \\; Entropy \\, - \\, New \\; Entropy)  \\over {New \\; Entropy}} $$\n",
    "\n",
    "IG Ratio는 Split이 많이 된 Tree에 대하여 Disadvantage를 줌으로써 Overfitting을 방지한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1-2. Overfitting 방지하기 : Pruning (가지치기)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사실, 위의 두 모델은 Overfitting된 것이다.\n",
    "\n",
    "Tree의 크기에 따라 모델의 성능이 좌우될 수 있다.\n",
    "\n",
    " - 크기가 너무 크면 Overfitting\n",
    " - 크기가 너무 작으면 Underfitting\n",
    "\n",
    "Overfitting, Underfitting을 방지하기 위하여 다음과 같은 방법을 사용할 수 있다.\n",
    "\n",
    "1) Information Gain 최솟값 설정\n",
    "\n",
    "2) 가지의 개수 제한\n",
    "\n",
    "3) 나무의 깊이 (Depth) 제한"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning에는 두 종류가 있다.\n",
    "\n",
    "#### 1-1-2-1. Pre-Pruning (Early Stopping)\n",
    "\n",
    "Grid Search 등의 Hyperparameter Tuning 기법 등을 통하여 나무의 깊이와 가지의 개수만 조정해주면 된다.\n",
    "\n",
    "#### 1-1-2-2. Post-Pruning\n",
    "학습 후 Pruning을 진행한다.\n",
    "\n",
    " - Cost-Complexity Pruning (CCP)이 가장 대중적\n",
    " - Error를 최소화하는 Alpha를 찾는다.\n",
    " \n",
    "1. Cost - Complexity Pruning (CCP)\n",
    " \n",
    "$$ C_{\\alpha}(T_k) = \\sum_{t = 1}^{|T_k|} N_t i(T_k) + \\alpha |T_k| \\quad \\; (k = 0, 1, ... , K-1, K)$$\n",
    "\n",
    "$$ T^{*} = arg\\min_{T_k} C_{\\alpha}(T_k) $$\n",
    "\n",
    "$$ T_K : 최대 \\; 크기의 \\; Tree $$\n",
    "$$ T_K > T_{K-1} > ... > T_0  : 연속적으로 \\; Pruning을 \\; 진행하였을 \\; 때의 \\; Tree의 \\; 집합 $$\n",
    "$$ |T_k| : T_k의 \\; Terminal \\; Node \\; 수$$\n",
    "\n",
    "가장 오른쪽 항 ($ \\alpha |T_k| $)은 나무의 크기에 따른 Penalty의 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "식을 간소화하여 살펴보자.\n",
    "\n",
    "$$ ① \\; R_\\alpha (t) = R(t) + \\alpha \\quad (노드 \\; t의 \\; Impurity ; \\; |T_t| = 1) $$\n",
    "$$ ② \\; R_\\alpha (T_t) = R(T_t) + \\alpha|T_t| \\quad (노드 \\; t가 \\; Terminal \\; Node인 \\; Tree \\; T_t의 \\; Impurity)$$\n",
    "\n",
    "$T_t$는 t보다 세분화된 모델이기 때문에, 일반적으로 $ R(T_t) < R(t)$ 이다.\n",
    "\n",
    "그러나, 모델이 지나치게 클 경우 Overfitting 문제가 발생하여 $ R(T_t) > R(t) $가 될 수 있으므로 크기에 적절히 penalty를 줄 수 있는 $ \\alpha $를 찾아야 한다.\n",
    "\n",
    "$ |T_t| >= 1 $이므로, $ \\alpha $가 커지다보면 $ R(T_t) = R(t) $가 되는 지점인 $ \\alpha^{*} $를 찾을 수 있다. \n",
    "\n",
    "이 때, $ \\alpha > \\alpha^{*} $가 되면 $ R(T_t) > R(t) $, 즉 Node $ t $에서 Split을 하였을 때 오히려 성능 저하가 생기게 되어 Pruning이 필요하게 된다. (분지 여부에 따른 Impurity의 차이가 없는 부분을 찾아 Pruning을 한다는 것이다.)\n",
    "\n",
    "$$ R_{\\alpha^{*}} (t) = R_{\\alpha^{*}} (T_t) $$\n",
    "\n",
    "$$ \\alpha^{*} = \\frac{R(t) - R(T_t)}{|T_t|} $$\n",
    "\n",
    "이를 만족하는 $ \\alpha^{*} $에 대응하는 노드를 Weakest Link라 하며, CCP는 Weakest Link Cut이라고도 불린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CCP의 장점\n",
    " - Impurity를 최대한 유지하면서도 과하지 않게 Pruning → Stable Model\n",
    "\n",
    "CCP의 단점 \n",
    " - 복잡한 알고리즘\n",
    " - 모든 Subtree를 고려하지 못함 → Local Minimum에 빠질 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Reduced - Error Pruning (REP)\n",
    "\n",
    "Validation Set을 이용하여 모든 중간 마디들(Branch Node)에 대하여\n",
    "\n",
    "split 전과 후의 Impurity 차이를 비교하여 Pruning을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REP의 장점\n",
    " - 직관적인 알고리즘\n",
    " \n",
    "REP의 단점\n",
    " - Validation Set의 크기가 작으면 OverPruning 가능성이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Rule Post-Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth = 2)\n",
    "clf4.fit(iris_x, iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data_3 = tree.export_graphviz(clf4\n",
    "                                 , out_file = None\n",
    "                                 , feature_names = iris.feature_names\n",
    "                                 , class_names = iris.target_names\n",
    "                                 , filled = True\n",
    "                                 , rounded = True\n",
    "                                 , special_characters = True\n",
    "                                )\n",
    "\n",
    "graph_3 = graphviz.Source(dot_data_3)\n",
    "graph_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_attributes = [item for item in dir(DecisionTreeClassifier) if item.startswith('_') == False]\n",
    "tree_attributes\n",
    "\n",
    "# 다른 코드로는...\n",
    "## tree_attributes = list(filter(lambda x : x.startswith(\"_\") == False, dir(DecisionTreeClassifier)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4.__dict__\n",
    "    # __dict__ : 객체의 속성 정보 확인 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1-3. 정확도 비교 : Confusion Matrix 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 나무 (Entropy)\n",
    "confusion_matrix(iris.target, clf2.predict(iris.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두번째 나무 (Gini Index)\n",
    "confusion_matrix(iris.target, clf3.predict(iris.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 세번째 나무 (Entropy & Pruning)\n",
    "confusion_matrix(iris.target, clf4.predict(iris.data))\n",
    "\n",
    "# 가지치기를 한 Tree의 정확도가 가장 떨어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1-4. 데이터 분할 : Train Set / Validation Set / Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 데이터는 크기가 비교적 작은 편이기 때문에, Validation Set을 따로 두지 않고 진행한다.\n",
    "\n",
    "iris_x_train, iris_x_test, iris_y_train, iris_y_test = train_test_split(iris.data, iris.target\n",
    "                                                                        , test_size = 0.3\n",
    "                                                                        , random_state = 123\n",
    "                                                                        , stratify = iris.target\n",
    "                                                                       )\n",
    "\n",
    "    # stratify : 층화추출 적용\n",
    "    # Iris 데이터는 완벽한 Balanced Data이기 때문에, 무작위 추출 시 Imbalanced Data로 변할 위험이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Set으로 다시 모델링\n",
    "clf5 = tree.DecisionTreeClassifier(criterion = 'entropy')\n",
    "clf5.fit(iris_x_train, iris_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(iris_y_test, clf5.predict(iris_x_test))\n",
    "    # 가지치기 없이도 오분류율이 높아짐.\n",
    "    # 데이터 분할 시 Training Set과 Test Set의 특성이 조금씩 달라지기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Decision Regression Tree : 의사결정회귀나무 (CART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(123)\n",
    "    # np.random.RandomState : 난수 생성기\n",
    "\n",
    "x = np.sort(5 * rng.rand(80, 1), axis = 0)\n",
    "    # np.random.RandomState.rand(m, n) : 균등분포 U(0, 1)에서 난수 Array (m, n) 생성\n",
    "    # np.random.randn(m, n) : Gaussian Normal Distribution N(0, 1)에서 난수 Array (m, n) 생성\n",
    "y = np.sin(x).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth가 다른 두 회귀트리 생성 & 적합\n",
    "reg1 = tree.DecisionTreeRegressor(max_depth = 2)\n",
    "    # 분리 기준 : MSE, MAE, Poisson Deviance (포아송 편차 감소도), Friedman's MSE \n",
    "reg2 = tree.DecisionTreeRegressor(max_depth = 5)\n",
    "\n",
    "reg1.fit(x, y)\n",
    "reg2.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = reg1.predict(x_test)\n",
    "y2 = reg2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 도식화\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(x, y\n",
    "           , s = 20\n",
    "           , edgecolor = \"black\"\n",
    "           , c = \"pink\"\n",
    "           , label = \"Data\")\n",
    "plt.plot(x_test, y1\n",
    "        , color = \"cornflowerblue\"\n",
    "        , label = \"Model 1 : 최대 깊이 2\"\n",
    "        , linewidth = 2)\n",
    "\n",
    "plt.plot(x_test, y2\n",
    "        , color = \"yellowgreen\"\n",
    "        , label = \"Model 2 : 최대 깊이 5\"\n",
    "        , linewidth = 2)\n",
    "\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Target\")\n",
    "plt.title(\"의사결정나무 회귀트리\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max_depth = 5인 나무가 이상치의 영향을 더 많이 받았음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
